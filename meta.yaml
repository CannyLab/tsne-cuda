{% set name = "tsnecuda" %}
{% set version = "2.1.0" %}

package:
  name: '{{ name|lower }}'
  version: '{{ version }}'

source:
  git_url: https://github.com/CannyLab/tsne-cuda.git
  git_rev: HEAD

build:
    features:
        - '{{ cuda_version }}'
    string: '{{ cuda_version }}'
    noarch_python: True

requirements:
  build:
      # - {{ compiler('cxx') }}
    - python
    - gcc_49
    - cmake
    - setuptools
    - libstdcxx-ng
    - gflags
    - glog
    - gtest
  host:
    - python
    - openblas
    - setuptools
    - numpy >=1.14.1
    - libopenblas
    - libgcc-ng
    - libstdcxx-ng
    - gflags
    - glog
    - gtest
  run:
    - python
    - openblas
    - numpy >=1.14.1
    - libopenblas
    - libgcc-ng
    - libstdcxx-ng
    - gflags
    - glog
    - gtest

about:
  home: https://github.com/CannyLab/tsne-cuda
  license: LICENSE.txt
  license_family: BSD
  license_file: ''
  summary: CUDA Implementation of T-SNE with Python bindings
  description: "===========\ntsnecuda\n===========\n\ntsnecuda provides an optimized CUDA implementation of the T-SNE algorithm by L Van der Maaten. tsnecuda is able to compute the T-SNE of large numbers\
    \ of points up to 1200 times faster than other leading libraries, and provides simple python bindings with a SKLearn style interface::\n\n    #!/usr/bin/env python\n\n    from tsnecuda import TSNE\n\
    \    embeddedX = TSNE(n_components=2).fit_transform(X)\n\nFor more information, check out the repository at https://github.com/rmrao/tsne-cuda. \n\n\n"
  doc_url: ''
  dev_url: ''

extra:
  recipe-maintainers: ''
